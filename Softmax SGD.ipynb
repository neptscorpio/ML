{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X: d x n np.ndarray\n",
    "# return a d x n ndarray\n",
    "def softmax(X):\n",
    "    X -= X.max(axis=0)  # for each column, substract the max over row\n",
    "    eX = np.exp(X) \n",
    "    return eX / eX.sum(axis=0)  # softmax for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return a 1xn predicted class label with highest softmax probabilities\n",
    "# X1: (d+1) x n, features of all samples \n",
    "# W1: c x (d+1), weights\n",
    "def predict_softmax(X1, W1):\n",
    "    Y_soft = softmax( W1.dot(X1) )  # c x n\n",
    "    y_hat = np.argmax( Y_soft, axis=0 )  # 1 x n\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return a LogLoss value between true label y and softmax probabilities\n",
    "# X: d x n, d features of n samples \n",
    "# y: 1 x n, class label in [0, c), dtype='uint8'\n",
    "# W: c x d, weights\n",
    "def loss_softmax(X, y, W):\n",
    "    Y_soft = softmax( W.dot(X) )  # c x n\n",
    "    y_prob = Y_soft[y, range(len(y))]  # 1 x n\n",
    "    #print \"len(y)=\", len(y)\n",
    "    return -np.log(y_prob).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use partial derivative df = (f(X+h) - f(X-h)) / 2h on each feature of X\n",
    "# func: a function that takes a single argument\n",
    "# X is the point (numpy array) to evaluate the gradient at\n",
    "# h: step\n",
    "def gradient(func, X, h=0.00001):\n",
    "    grad = np.zeros(X.shape)\n",
    "    # iterate over all indexes in X\n",
    "    it = np.nditer(X, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        # evaluate function at x+h\n",
    "        ix = it.multi_index\n",
    "        old_value = X[ix]\n",
    "        X[ix] = old_value + h \n",
    "        fxh1 = func(X) # evalute f(x + h)\n",
    "        #print X, fxh1\n",
    "        X[ix] = old_value - h \n",
    "        fxh2 = func(X) # evalute f(x - h)\n",
    "        #print X, fxh2\n",
    "        X[ix] = old_value # restore to previous value (very important!)\n",
    "\n",
    "        # compute the partial derivative\n",
    "        grad[ix] = (fxh1 - fxh2) / (2*h) # the slope\n",
    "        it.iternext() # step to next dimension\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(-16.000000000282455)"
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## test 1\n",
    "f = lambda x: -3*x*x + 2*x + 4\n",
    "X = np.array(3.0)\n",
    "gradient( f, X )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.,  3.])"
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test 2\n",
    "f = lambda X: X[0]*X[1]\n",
    "X = np.array([3.0, 4.0])\n",
    "gradient( f, X )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = 8 # feature dimension\n",
    "n = 100 # sample size\n",
    "c = 5 # num of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1122)\n",
    "X = np.random.randn(d, n) # features\n",
    "vec1 = np.ones(n)\n",
    "X1 = np.vstack((X, vec1)) # extended features including intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True weights to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1122)\n",
    "W1 = np.random.randn(c, d+1)   # true weights, including intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "noise = np.random.normal(0, 0.1, (c, n))\n",
    "Y_soft = softmax( W1.dot(X1) + noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = np.argmax( Y_soft, axis=0 )   # true class labels, in [0, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.95612339852844"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_softmax(X1, y, W1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function on weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## given training data (X1 and y), loss is func of W1 only\n",
    "def loss_on_weights( W1 ):\n",
    "    return loss_softmax(X1, y, W1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1122)\n",
    "W1_hat = np.random.randn(c, d+1) * 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Decendent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: loss = 159.797967\n",
      "iter 100: loss = 158.935277\n"
     ]
    }
   ],
   "source": [
    "## Note: this code is generic of loss functions\n",
    "iter = 0\n",
    "step = 10 ** -5\n",
    "while 1:\n",
    "    loss_new = loss_on_weights( W1_hat )\n",
    "    grad_W1_hat = gradient( loss_on_weights, W1 )\n",
    "    W1_hat += - step * grad_W1_hat\n",
    "    if iter % 100 == 0:\n",
    "        print \"iter %d: loss = %f\" % ( iter, loss_new )\n",
    "    iter += 1;\n",
    "    if iter >= 1001:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0, -2,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  1,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predict(X1, W1) - y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3,  0,  0,  0,  0, -2, -3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  1,  0,  0,  0,  0,  0, -3,  3,  4,  0,  1,  0,  4, -2,  1,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  4,  0,  0,  0, -1,  3,\n",
       "        1, -2,  0,  1,  1,  0,  0, -2, -2,  0,  1,  1,  0,  0,  0,  3,  2,\n",
       "        0,  0, -1,  0,  0,  1,  1,  0, -3,  1,  0,  0,  1,  0,  0,  0,  3,\n",
       "        0, -2,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1])"
      ]
     },
     "execution_count": 590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(X1, W1_hat) - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
