{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import itertools\n",
    "#nltk.download('all')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# softmax: take each COLUMN as a score vector\n",
    "# X: c x n np.ndarray\n",
    "# return a c x n ndarray\n",
    "# Caveat: if X is 1 * n array, it will apply on \"row\" still\n",
    "def softmax_by_col(X):\n",
    "    M = X.copy()\n",
    "    M -= M.max(axis=0)  # for each column, substract the max over row\n",
    "    eM = np.exp(M) \n",
    "    return eM / eM.sum(axis=0)  # softmax for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Caveat: if X is 1 * n array, it will return the same 1 * n array\n",
    "def softmax(X, by='col'):\n",
    "    if by == 'col':\n",
    "        return softmax_by_col(X)\n",
    "    elif by == 'row':\n",
    "        return softmax_by_col(X.T).T\n",
    "    else:\n",
    "        raise        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# y: nSamp x dim, each row is the real prob.\n",
    "# y_pred: nSamp x dim, each row is the predicted prob.\n",
    "# return: sum loss for all samples\n",
    "def sum_log_loss(y, y_pred):\n",
    "    return - ( y * np.log(y_pred + 1e-9) ).sum(axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.38521473638\n",
      "0.105360515658\n"
     ]
    }
   ],
   "source": [
    "y = np.asarray([[0., 1., 0., 0.], [0.3, 0.4, 0.2, 0.1]])\n",
    "y_pred = np.asarray([[0.1, 0.9, 0., 0.], [0.3, 0.4, 0.2, 0.1]])\n",
    "print sum_log_loss(y, y_pred)\n",
    "print -np.log(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x is a sequence/list of index (index in 0 - dim-1), 1 * n\n",
    "# return: e, n * dim\n",
    "def one_hot_encode(x, dim):\n",
    "    #print x, dim\n",
    "    n = len(x)\n",
    "    #print n\n",
    "    e = np.zeros( (n, dim) )\n",
    "    e[xrange(n), x] += 1\n",
    "    #print e.shape\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# e: an one-hot code, e.g. [0, 0, 1, 0]\n",
    "# return: index, e.g. 2\n",
    "def one_hot_decode(e):\n",
    "    return np.argmax(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voc_sz = 2000\n",
    "unknown = \"#\"\n",
    "sent_start = \"^\"\n",
    "sent_end = \"@\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training file...\n"
     ]
    }
   ],
   "source": [
    "# Read the data and append SENTENCE_START and SENTENCE_END tokens\n",
    "print \"Reading training file...\"\n",
    "with open('sayings.txt', 'rb') as f:\n",
    "    # body = f.read()\n",
    "    sents = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sents = [s.lower().rstrip() for s in sents if len(s)>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sents = [\"%s %s %s\" % (sent_start, s, sent_end) for s in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 1129 sentences.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'^ a cat may look at a king @'"
      ]
     },
     "execution_count": 609,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"Parsed %d sentences.\" % (len(sentences))\n",
    "sents[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567  tokenized sentences.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the sentences into words\n",
    "sents_t = [nltk.word_tokenize(s) for s in sents]  # a list of sent_t, i.e. list of words\n",
    "print len(sents_t), \" tokenized sentences.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example sentence: ^ a bad penny always turns up @\n",
      "\n",
      "Example sentence after Pre-processing: ['^', 'work', 'expands', 'so', 'as', 'to', 'fill', 'the', 'time', 'available', '@']\n"
     ]
    }
   ],
   "source": [
    "print \"\\nExample sentence: %s\" % sents[0]\n",
    "print \"\\nExample sentence after Pre-processing: %s\" % sents_t[546]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['^',\n",
       " 'work',\n",
       " 'expands',\n",
       " 'so',\n",
       " 'as',\n",
       " 'to',\n",
       " 'fill',\n",
       " 'the',\n",
       " 'time',\n",
       " 'available',\n",
       " '@']"
      ]
     },
     "execution_count": 614,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_t[546]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*sents_t)) # a FreqDist object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example word_freq itesm:  [('stones', 3), ('all', 32), ('forget', 1)]\n",
      "Found 1196 unique words tokens.\n"
     ]
    }
   ],
   "source": [
    "print \"Example word_freq itesm: \", word_freq.items()[:3]\n",
    "print \"Found %d unique words tokens.\" % len(word_freq.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voc_sz =  1196\n"
     ]
    }
   ],
   "source": [
    "voc_sz = min( [voc_sz, len(word_freq.items())] )\n",
    "print \"voc_sz = \", voc_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('^', 567), ('@', 567), ('the', 239), ('a', 183), ('is', 142), ('to', 84), (',', 84), ('you', 72), (\"'s\", 69), ('and', 68)]\n"
     ]
    }
   ],
   "source": [
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common( voc_sz ) # a list of tuple (word, cnt)\n",
    "print vocab[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word -> an index in vocab, index in [0, voc_sz)\n",
    "word_to_idx = dict([(w, idx) for idx, (w, cnt) in enumerate(vocab)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 622,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx['^']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the training data\n",
    "# X: each row is a training sample, i.e., an input sequence, or, i.e., a sentence. Each word is an index (0 - voc_sz-1)\n",
    "# y: each row is a output, i.e., a shift-1-word sequence\n",
    "X_train = np.asarray([ [word_to_idx[w] for w in sent_t[:-1]] for sent_t in sents_t ])  # n sents_t\n",
    "Y_train = np.asarray([ [word_to_idx[w] for w in sent_t[1:]] for sent_t in sents_t ])   # n sents_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 3, 92, 136, 47, 511, 135], [0, 3, 436, 91, 19, 617],\n",
       "       [0, 3, 182, 14, 2, 160, 4, 68, 69, 14, 2, 853],\n",
       "       [0, 3, 145, 71, 231, 57, 3, 383],\n",
       "       [0, 3, 484, 4, 98, 16, 553, 16, 72, 1020, 1133]], dtype=object)"
      ]
     },
     "execution_count": 624,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 92, 136, 47, 511, 135, 1], [3, 436, 91, 19, 617, 1],\n",
       "       [3, 182, 14, 2, 160, 4, 68, 69, 14, 2, 853, 1],\n",
       "       [3, 145, 71, 231, 57, 3, 383, 1],\n",
       "       [3, 484, 4, 98, 16, 553, 16, 72, 1020, 1133, 1]], dtype=object)"
      ]
     },
     "execution_count": 625,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN:\n",
    "     def __init__(self, dim, dim_h=100, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.dim = dim # dim. of input/output vec, e.g. the embedding vec. of a word in a sequence\n",
    "        self.dim_h = dim_h # hidden layer\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # U: input vec to hidden states, dim * dim_h, (so we can dot(x, U))\n",
    "        self.U = np.random.randn( dim, dim_h ) * np.sqrt( 2. / ( dim * dim_h ) )\n",
    "        # V: hidden states to output vec, dim_h * dim (so we can dot(V, h) )\n",
    "        self.V = np.random.randn( dim_h, dim ) * np.sqrt( 2. / ( dim_h * dim ) )\n",
    "        # W: (prev) hidden state to (next) hidden state \n",
    "        self.W = np.random.randn( dim_h, dim_h ) * np.sqrt( 2. / ( dim_h * dim_h ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# x: T * dim, a input sequence of vectors, [x1, x2, ..., x_t], each row is a 1 * dim vector\n",
    "# return: o, output softmax scores, T * dim\n",
    "#         s, hidden states, (T+1) * dim_h. s[-1] is initial state, all zeros\n",
    "def forwardProp(self, x):\n",
    "    T = len(x)\n",
    "    # Save all hidden states for bptt.\n",
    "    # Add one initial hidden state, which are 0\n",
    "    s = np.zeros( (T + 1, self.dim_h) ) # s[-1] are zeros (1 * dim_h)\n",
    "    o = np.zeros( (T, self.dim) )\n",
    "    for t in xrange(T):\n",
    "#         print \"t = \", t\n",
    "#         print \"x[t]\", x[t]\n",
    "#         print \"x[t].shap\", x[t].shape\n",
    "#         print \"U\", self.U.shape\n",
    "#         print \"s\", s[t-1].shape\n",
    "#         print \"W\", self.W.shape\n",
    "        s[t] = np.tanh( np.dot( x[t], self.U ) + np.dot( s[t-1], self.W ) ) # 1 * dim_h\n",
    "        o[t] = softmax( np.dot( s[t], self.V ), by='row' )  # 1 * dim \n",
    "    return [o, s]\n",
    "RNN.forwardProp = forwardProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x: T * dim, a input sequence of vectors, [x1, x2, ..., x_t], each row is a 1 * dim vector\n",
    "# return: labels, 1 * T, \n",
    "#         For each step t, label is the index of the dimension with max value in the output softmax score,\n",
    "def predict(self, x):\n",
    "    o, s = self.forwardProp(x)\n",
    "    return np.argmax(o, axis=1)\n",
    "RNN.predict = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X: a batch of n sequences, each sequence is a T * dim array. T may vary by sequence\n",
    "# Y: a batch of n sentences, each sequence is a T * dim array. T may vary by sequence\n",
    "# return: the sum log loss on all sequences\n",
    "def total_loss(self, X, Y):\n",
    "    n = len(X) # num. of training sequences\n",
    "    sum_loss = 0.\n",
    "    for i in xrange(n): # for each sequence\n",
    "        x = X[i]  # a sequence, T * dim\n",
    "        y = Y[i]  # a sequence, T * dim\n",
    "        o, s = self.forwardProp( x ) # T * dim\n",
    "        loss = sum_log_loss( y, o ) # sum loss in this sequence\n",
    "        sum_loss += loss\n",
    "    return sum_loss  # sum loss on all sequences of vectors\n",
    "RNN.total_loss = total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X: a batch of n sequences, each sequence is a T * dim array. T may vary by sequence\n",
    "# Y: a batch of n sentences, each sequence is a T * dim array. T may vary by sequence\n",
    "# return: the average log loss for each vector\n",
    "def loss(self, X, Y):\n",
    "    n = len(X) # num. of training sequences\n",
    "    sum_loss = 0.\n",
    "    for i in xrange(n): # for each sequence\n",
    "        x = X[i]  # a sequence, T * dim\n",
    "        #print x.shape\n",
    "        y = Y[i]  # a sequence, T * dim\n",
    "        #print y.shape\n",
    "        o, s = self.forwardProp( x ) # T * dim\n",
    "        #print o.shape\n",
    "        loss = sum_log_loss( y, o ) # sum loss in this sequence\n",
    "        #print loss\n",
    "        sum_loss += loss\n",
    "    n_vec = np.sum( [len(y) for y in Y] )  \n",
    "    #print n_vec\n",
    "    return sum_loss / n_vec  # avg. loss per vector\n",
    "RNN.loss = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### !!!  seeems only for one-hot encoding \n",
    "# x: a sequence of vec, T * dim\n",
    "# y: a sequence of vec, T * dim\n",
    "def bptt(self, x, y):\n",
    "    T = len(y)\n",
    "    o, s = self.forwardProp(x)  # o: T * dim,  s: T * dim_h\n",
    "    dU = np.zeros_like(self.U)  # dU is actually dL/dU\n",
    "    dV = np.zeros_like(self.V)\n",
    "    dW = np.zeros_like(self.W)\n",
    "    idx_x = np.argmax(x, axis=1) # 1 * T, each entry is an index\n",
    "    idx_y = np.argmax(y, axis=1) # 1 * T, each entry is an index\n",
    "    delta_o = o  # T * dim\n",
    "    delta_o[np.arange(len(idx_y)), idx_y] -= 1.\n",
    "    # For each output backwards...\n",
    "    for t in np.arange(T)[::-1]:\n",
    "        #print dV.shape  # dim_h * dim\n",
    "        dV += np.outer(s[t].T, delta_o[t])\n",
    "        # Initial delta calculation\n",
    "        delta_t = np.dot(delta_o[t], self.V.T) * (1 - (s[t] ** 2))   \n",
    "        # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "        steps = np.arange(max(0, t-self.bptt_truncate), t+1)\n",
    "        for bptt_step in steps[::-1]:\n",
    "            dW += np.outer(s[bptt_step-1].T, delta_t)              \n",
    "            dU[idx_x[bptt_step], :] += delta_t\n",
    "            # Update delta for next step\n",
    "            delta_t = np.dot(delta_t, self.W.T) * (1 - s[bptt_step-1] ** 2)\n",
    "    return [dU, dV, dW]\n",
    "RNN.bptt = bptt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "# x: a sequence of vec, T * dim\n",
    "# y: a sequence of vec, T * dim\n",
    "def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "    # Calculate the gradients using backpropagation. We want to checker if these are correct.\n",
    "    bptt_gradients = self.bptt(x, y)\n",
    "    # List of all parameters we want to check.\n",
    "    model_parameters = ['U', 'V', 'W']\n",
    "    # Gradient check for each parameter\n",
    "    for pidx, pname in enumerate(model_parameters):\n",
    "        # Get the actual parameter value from the mode, e.g. model.W\n",
    "        parameter = operator.attrgetter(pname)(self)\n",
    "        print \"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape))\n",
    "        # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "        it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            # Save the original value so we can reset it later\n",
    "            original_value = parameter[ix]\n",
    "            # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "            parameter[ix] = original_value + h\n",
    "            #print [x].shape\n",
    "            #print [y].shape\n",
    "            gradplus = self.total_loss([x],[y])\n",
    "            parameter[ix] = original_value - h\n",
    "            gradminus = self.total_loss([x],[y])\n",
    "            estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "            # Reset parameter to original value\n",
    "            parameter[ix] = original_value\n",
    "            # The gradient for this parameter calculated using backpropagation\n",
    "            backprop_gradient = bptt_gradients[pidx][ix]\n",
    "            # calculate The relative error: (|x - y|/(|x| + |y|))\n",
    "            relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "            # If the error is to large fail the gradient check\n",
    "            if relative_error >= error_threshold:\n",
    "                print \"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix)\n",
    "                print \"+h Loss: %f\" % gradplus\n",
    "                print \"-h Loss: %f\" % gradminus\n",
    "                print \"Estimated_gradient: %f\" % estimated_gradient\n",
    "                print \"Backpropagation gradient: %f\" % backprop_gradient\n",
    "                print \"Relative Error: %f\" % relative_error\n",
    "                return\n",
    "            it.iternext()\n",
    "        print \"Gradient check for parameter %s passed.\" % (pname)\n",
    "RNN.gradient_check = gradient_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter U with size 50.\n",
      "Gradient check for parameter U passed.\n",
      "Performing gradient check for parameter V with size 50.\n",
      "Gradient check for parameter V passed.\n",
      "Performing gradient check for parameter W with size 25.\n",
      "Gradient check for parameter W passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sxue/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:33: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "# To avoid performing millions of expensive calculations we use a smaller vocabulary size for checking.\n",
    "grad_check_vocab_size = 10\n",
    "np.random.seed(10)\n",
    "model = RNN(grad_check_vocab_size, 5, bptt_truncate=1000)\n",
    "\n",
    "#print \"U\", model.U.shape\n",
    "model.gradient_check( one_hot_encode( [0,1,2,3], grad_check_vocab_size), \n",
    "                      one_hot_encode( [1,2,3,4], grad_check_vocab_size)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Performs one step of SGD on 1 training sequence\n",
    "# x: a sequence of vec, T * dim\n",
    "# y: a sequence of vec, T * dim\n",
    "def sgd_update(self, x, y, step):\n",
    "    # Calculate the gradients\n",
    "    dU, dV, dW = self.bptt(x, y)\n",
    "    # Change parameters according to gradients and learning rate\n",
    "    self.U += - step * dU\n",
    "    self.V += - step * dV\n",
    "    self.W += - step * dW\n",
    "RNN.sgd_update = sgd_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# Training Loop\n",
    "# - X_train: The training data set, batch of sequences\n",
    "# - y_train: The training data labels, batch\n",
    "# - step: learning rate for SGD\n",
    "# - nepoch: Number of times to iterate through the complete dataset\n",
    "def train(self, X_train_encode, Y_train_encode, step=0.005, nepoch=100):\n",
    "    losses = []     # keep track of the losses\n",
    "    n_sample_seen = 0  # samples (sequences) have seen\n",
    "    for epoch in range(nepoch):\n",
    "        if (epoch % 5 == 0):\n",
    "            loss = self.loss(X_train_encode, Y_train_encode)\n",
    "            losses.append( (n_sample_seen, loss) )\n",
    "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print \"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, n_sample_seen, epoch, loss)\n",
    "            # Adjust the learning rate if loss increases\n",
    "            if ( len(losses) >= 2 and losses[-1][1] >= losses[-2][1]):\n",
    "                step *= 0.5 \n",
    "                print \"Setting learning rate to %f\" % step\n",
    "\n",
    "        n = len(Y_train_encode)\n",
    "        for i in xrange(n): # train sample by sample, i.e. batch=1\n",
    "            self.sgd_update(X_train_encode[i], Y_train_encode[i], step)\n",
    "            n_sample_seen += 1\n",
    "RNN.train = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1196\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "print voc_sz\n",
    "rnn = RNN(dim=voc_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(567,)\n",
      "(567,)\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape\n",
    "print Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_encode = []\n",
    "for x in X_train: # a sequence of indices\n",
    "    x_encode = one_hot_encode( x, voc_sz ) # T * dim\n",
    "    X_train_encode.append( x_encode )\n",
    "Y_train_encode = []\n",
    "for y in Y_train: # a sequence of indices\n",
    "    y_encode = one_hot_encode( y, voc_sz ) # T * dim\n",
    "    Y_train_encode.append( y_encode )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 7.15 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit rnn.sgd_update(X_train_encode[10], Y_train_encode[10], step=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-11-12 15:58:20: Loss after num_examples_seen=0 epoch=0: 2.966479\n",
      "2016-11-12 15:58:43: Loss after num_examples_seen=2835 epoch=5: 3.335902\n",
      "Setting learning rate to 0.002500\n",
      "2016-11-12 15:59:06: Loss after num_examples_seen=5670 epoch=10: 2.890300\n",
      "2016-11-12 15:59:27: Loss after num_examples_seen=8505 epoch=15: 2.768825\n",
      "2016-11-12 15:59:49: Loss after num_examples_seen=11340 epoch=20: 2.743475\n",
      "2016-11-12 16:00:11: Loss after num_examples_seen=14175 epoch=25: 2.741667\n",
      "2016-11-12 16:00:36: Loss after num_examples_seen=17010 epoch=30: 2.739850\n",
      "2016-11-12 16:00:59: Loss after num_examples_seen=19845 epoch=35: 2.707654\n",
      "2016-11-12 16:01:21: Loss after num_examples_seen=22680 epoch=40: 2.695286\n",
      "2016-11-12 16:01:43: Loss after num_examples_seen=25515 epoch=45: 2.678172\n",
      "2016-11-12 16:02:09: Loss after num_examples_seen=28350 epoch=50: 2.640412\n",
      "2016-11-12 16:02:31: Loss after num_examples_seen=31185 epoch=55: 2.603842\n",
      "2016-11-12 16:02:54: Loss after num_examples_seen=34020 epoch=60: 2.599846\n",
      "2016-11-12 16:03:18: Loss after num_examples_seen=36855 epoch=65: 2.602250\n",
      "Setting learning rate to 0.001250\n",
      "2016-11-12 16:03:57: Loss after num_examples_seen=39690 epoch=70: 2.517504\n",
      "2016-11-12 16:04:30: Loss after num_examples_seen=42525 epoch=75: 2.501725\n",
      "2016-11-12 16:04:52: Loss after num_examples_seen=45360 epoch=80: 2.493290\n",
      "2016-11-12 16:05:16: Loss after num_examples_seen=48195 epoch=85: 2.484547\n",
      "2016-11-12 16:05:36: Loss after num_examples_seen=51030 epoch=90: 2.473347\n",
      "2016-11-12 16:05:56: Loss after num_examples_seen=53865 epoch=95: 2.461600\n",
      "2016-11-12 16:06:18: Loss after num_examples_seen=56700 epoch=100: 2.449131\n",
      "2016-11-12 16:06:39: Loss after num_examples_seen=59535 epoch=105: 2.436429\n",
      "2016-11-12 16:06:59: Loss after num_examples_seen=62370 epoch=110: 2.422501\n",
      "2016-11-12 16:07:19: Loss after num_examples_seen=65205 epoch=115: 2.411616\n",
      "2016-11-12 16:07:39: Loss after num_examples_seen=68040 epoch=120: 2.401242\n",
      "2016-11-12 16:08:00: Loss after num_examples_seen=70875 epoch=125: 2.392723\n",
      "2016-11-12 16:08:20: Loss after num_examples_seen=73710 epoch=130: 2.382288\n",
      "2016-11-12 16:08:40: Loss after num_examples_seen=76545 epoch=135: 2.372628\n",
      "2016-11-12 16:09:00: Loss after num_examples_seen=79380 epoch=140: 2.361524\n",
      "2016-11-12 16:09:20: Loss after num_examples_seen=82215 epoch=145: 2.349176\n",
      "2016-11-12 16:09:42: Loss after num_examples_seen=85050 epoch=150: 2.336923\n",
      "2016-11-12 16:10:04: Loss after num_examples_seen=87885 epoch=155: 2.331128\n",
      "2016-11-12 16:10:24: Loss after num_examples_seen=90720 epoch=160: 2.328670\n",
      "2016-11-12 16:10:45: Loss after num_examples_seen=93555 epoch=165: 2.324864\n",
      "2016-11-12 16:11:05: Loss after num_examples_seen=96390 epoch=170: 2.321332\n",
      "2016-11-12 16:11:25: Loss after num_examples_seen=99225 epoch=175: 2.314949\n",
      "2016-11-12 16:11:45: Loss after num_examples_seen=102060 epoch=180: 2.305011\n",
      "2016-11-12 16:12:05: Loss after num_examples_seen=104895 epoch=185: 2.298637\n",
      "2016-11-12 16:12:26: Loss after num_examples_seen=107730 epoch=190: 2.295212\n",
      "2016-11-12 16:12:46: Loss after num_examples_seen=110565 epoch=195: 2.218908\n"
     ]
    }
   ],
   "source": [
    "rnn.train(X_train_encode, Y_train_encode, step=0.005, nepoch=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-11-12 16:13:06: Loss after num_examples_seen=0 epoch=0: 2.230186\n",
      "2016-11-12 16:13:26: Loss after num_examples_seen=2835 epoch=5: 3.806476\n",
      "Setting learning rate to 0.002500\n",
      "2016-11-12 16:13:46: Loss after num_examples_seen=5670 epoch=10: 2.865893\n",
      "2016-11-12 16:14:06: Loss after num_examples_seen=8505 epoch=15: 2.388203\n",
      "2016-11-12 16:14:27: Loss after num_examples_seen=11340 epoch=20: 2.309785\n",
      "2016-11-12 16:14:47: Loss after num_examples_seen=14175 epoch=25: 2.165267\n",
      "2016-11-12 16:15:07: Loss after num_examples_seen=17010 epoch=30: 2.114324\n",
      "2016-11-12 16:15:26: Loss after num_examples_seen=19845 epoch=35: 2.071835\n",
      "2016-11-12 16:15:47: Loss after num_examples_seen=22680 epoch=40: 2.051029\n",
      "2016-11-12 16:16:06: Loss after num_examples_seen=25515 epoch=45: 2.023273\n",
      "2016-11-12 16:16:26: Loss after num_examples_seen=28350 epoch=50: 2.017663\n",
      "2016-11-12 16:16:46: Loss after num_examples_seen=31185 epoch=55: 2.006291\n",
      "2016-11-12 16:17:05: Loss after num_examples_seen=34020 epoch=60: 2.106739\n",
      "Setting learning rate to 0.001250\n",
      "2016-11-12 16:17:25: Loss after num_examples_seen=36855 epoch=65: 1.915404\n",
      "2016-11-12 16:17:45: Loss after num_examples_seen=39690 epoch=70: 1.888591\n",
      "2016-11-12 16:18:05: Loss after num_examples_seen=42525 epoch=75: 1.873194\n",
      "2016-11-12 16:18:25: Loss after num_examples_seen=45360 epoch=80: 1.864535\n",
      "2016-11-12 16:18:45: Loss after num_examples_seen=48195 epoch=85: 1.858480\n",
      "2016-11-12 16:19:04: Loss after num_examples_seen=51030 epoch=90: 1.846752\n",
      "2016-11-12 16:19:24: Loss after num_examples_seen=53865 epoch=95: 1.838978\n",
      "2016-11-12 16:19:44: Loss after num_examples_seen=56700 epoch=100: 1.831514\n",
      "2016-11-12 16:20:04: Loss after num_examples_seen=59535 epoch=105: 1.826089\n",
      "2016-11-12 16:20:23: Loss after num_examples_seen=62370 epoch=110: 1.819990\n",
      "2016-11-12 16:20:43: Loss after num_examples_seen=65205 epoch=115: 1.815508\n",
      "2016-11-12 16:21:02: Loss after num_examples_seen=68040 epoch=120: 1.809210\n",
      "2016-11-12 16:21:22: Loss after num_examples_seen=70875 epoch=125: 1.800201\n",
      "2016-11-12 16:21:42: Loss after num_examples_seen=73710 epoch=130: 1.789553\n",
      "2016-11-12 16:22:01: Loss after num_examples_seen=76545 epoch=135: 1.779975\n",
      "2016-11-12 16:22:21: Loss after num_examples_seen=79380 epoch=140: 1.770394\n",
      "2016-11-12 16:22:41: Loss after num_examples_seen=82215 epoch=145: 1.762885\n",
      "2016-11-12 16:23:00: Loss after num_examples_seen=85050 epoch=150: 1.755506\n",
      "2016-11-12 16:23:20: Loss after num_examples_seen=87885 epoch=155: 1.746445\n",
      "2016-11-12 16:23:40: Loss after num_examples_seen=90720 epoch=160: 1.738560\n",
      "2016-11-12 16:23:59: Loss after num_examples_seen=93555 epoch=165: 1.731695\n",
      "2016-11-12 16:24:19: Loss after num_examples_seen=96390 epoch=170: 1.723366\n",
      "2016-11-12 16:24:39: Loss after num_examples_seen=99225 epoch=175: 1.715141\n",
      "2016-11-12 16:24:59: Loss after num_examples_seen=102060 epoch=180: 1.709613\n",
      "2016-11-12 16:25:19: Loss after num_examples_seen=104895 epoch=185: 1.701814\n",
      "2016-11-12 17:25:34: Loss after num_examples_seen=107730 epoch=190: 1.695185\n",
      "2016-11-12 17:25:57: Loss after num_examples_seen=110565 epoch=195: 1.689544\n",
      "2016-11-12 18:26:16: Loss after num_examples_seen=113400 epoch=200: 1.685800\n",
      "2016-11-12 18:26:38: Loss after num_examples_seen=116235 epoch=205: 1.705554\n",
      "Setting learning rate to 0.000625\n",
      "2016-11-12 18:27:00: Loss after num_examples_seen=119070 epoch=210: 1.618836\n",
      "2016-11-12 18:27:23: Loss after num_examples_seen=121905 epoch=215: 1.602590\n",
      "2016-11-12 18:27:45: Loss after num_examples_seen=124740 epoch=220: 1.591308\n",
      "2016-11-12 18:28:07: Loss after num_examples_seen=127575 epoch=225: 1.582519\n",
      "2016-11-12 19:28:26: Loss after num_examples_seen=130410 epoch=230: 1.573852\n",
      "2016-11-12 19:28:49: Loss after num_examples_seen=133245 epoch=235: 1.565932\n",
      "2016-11-12 19:35:35: Loss after num_examples_seen=136080 epoch=240: 1.558351\n",
      "2016-11-12 19:36:01: Loss after num_examples_seen=138915 epoch=245: 1.552087\n",
      "2016-11-12 19:36:28: Loss after num_examples_seen=141750 epoch=250: 1.541050\n",
      "2016-11-12 19:36:54: Loss after num_examples_seen=144585 epoch=255: 1.535019\n",
      "2016-11-12 19:37:15: Loss after num_examples_seen=147420 epoch=260: 1.525646\n",
      "2016-11-12 19:37:41: Loss after num_examples_seen=150255 epoch=265: 1.522511\n",
      "2016-11-12 19:38:06: Loss after num_examples_seen=153090 epoch=270: 1.514981\n",
      "2016-11-12 19:38:34: Loss after num_examples_seen=155925 epoch=275: 1.508925\n",
      "2016-11-12 19:38:59: Loss after num_examples_seen=158760 epoch=280: 1.502489\n",
      "2016-11-12 19:39:26: Loss after num_examples_seen=161595 epoch=285: 1.495047\n",
      "2016-11-12 19:39:53: Loss after num_examples_seen=164430 epoch=290: 1.489089\n",
      "2016-11-12 19:40:17: Loss after num_examples_seen=167265 epoch=295: 1.484923\n",
      "2016-11-12 19:40:38: Loss after num_examples_seen=170100 epoch=300: 1.478674\n",
      "2016-11-12 19:41:02: Loss after num_examples_seen=172935 epoch=305: 1.473245\n",
      "2016-11-12 19:41:26: Loss after num_examples_seen=175770 epoch=310: 1.468374\n",
      "2016-11-12 19:41:50: Loss after num_examples_seen=178605 epoch=315: 1.464321\n",
      "2016-11-12 19:42:20: Loss after num_examples_seen=181440 epoch=320: 1.457778\n",
      "2016-11-12 19:42:52: Loss after num_examples_seen=184275 epoch=325: 1.453379\n",
      "2016-11-12 19:43:13: Loss after num_examples_seen=187110 epoch=330: 1.446285\n",
      "2016-11-12 19:43:35: Loss after num_examples_seen=189945 epoch=335: 1.438828\n",
      "2016-11-12 19:44:02: Loss after num_examples_seen=192780 epoch=340: 1.433268\n",
      "2016-11-12 19:44:25: Loss after num_examples_seen=195615 epoch=345: 1.432020\n",
      "2016-11-12 19:44:46: Loss after num_examples_seen=198450 epoch=350: 1.428929\n",
      "2016-11-12 19:45:11: Loss after num_examples_seen=201285 epoch=355: 1.422466\n",
      "2016-11-12 19:45:38: Loss after num_examples_seen=204120 epoch=360: 1.416580\n",
      "2016-11-12 19:46:06: Loss after num_examples_seen=206955 epoch=365: 1.408875\n",
      "2016-11-12 19:46:32: Loss after num_examples_seen=209790 epoch=370: 1.399839\n",
      "2016-11-12 19:46:57: Loss after num_examples_seen=212625 epoch=375: 1.393741\n",
      "2016-11-12 19:47:27: Loss after num_examples_seen=215460 epoch=380: 1.392168\n",
      "2016-11-12 19:47:47: Loss after num_examples_seen=218295 epoch=385: 1.387758\n",
      "2016-11-12 19:48:09: Loss after num_examples_seen=221130 epoch=390: 1.381877\n",
      "2016-11-12 19:48:31: Loss after num_examples_seen=223965 epoch=395: 1.371527\n",
      "2016-11-12 19:49:00: Loss after num_examples_seen=226800 epoch=400: 1.362436\n",
      "2016-11-12 19:49:24: Loss after num_examples_seen=229635 epoch=405: 1.354417\n",
      "2016-11-12 19:49:50: Loss after num_examples_seen=232470 epoch=410: 1.348158\n",
      "2016-11-12 19:50:17: Loss after num_examples_seen=235305 epoch=415: 1.344790\n",
      "2016-11-12 19:50:47: Loss after num_examples_seen=238140 epoch=420: 1.335591\n",
      "2016-11-12 19:51:16: Loss after num_examples_seen=240975 epoch=425: 1.330721\n",
      "2016-11-12 19:51:46: Loss after num_examples_seen=243810 epoch=430: 1.325784\n",
      "2016-11-12 19:52:14: Loss after num_examples_seen=246645 epoch=435: 1.315999\n",
      "2016-11-12 19:52:43: Loss after num_examples_seen=249480 epoch=440: 1.311320\n",
      "2016-11-12 19:53:12: Loss after num_examples_seen=252315 epoch=445: 1.305004\n",
      "2016-11-12 19:53:34: Loss after num_examples_seen=255150 epoch=450: 1.300704\n",
      "2016-11-12 19:53:57: Loss after num_examples_seen=257985 epoch=455: 1.295790\n",
      "2016-11-12 19:54:23: Loss after num_examples_seen=260820 epoch=460: 1.287897\n",
      "2016-11-12 19:54:54: Loss after num_examples_seen=263655 epoch=465: 1.281575\n",
      "2016-11-12 19:55:18: Loss after num_examples_seen=266490 epoch=470: 1.275577\n",
      "2016-11-12 19:55:45: Loss after num_examples_seen=269325 epoch=475: 1.270794\n",
      "2016-11-12 19:56:14: Loss after num_examples_seen=272160 epoch=480: 1.266950\n",
      "2016-11-12 19:56:42: Loss after num_examples_seen=274995 epoch=485: 1.264132\n",
      "2016-11-12 19:57:09: Loss after num_examples_seen=277830 epoch=490: 1.261158\n",
      "2016-11-12 19:57:39: Loss after num_examples_seen=280665 epoch=495: 1.257632\n",
      "2016-11-12 19:58:08: Loss after num_examples_seen=283500 epoch=500: 1.255054\n",
      "2016-11-12 19:58:32: Loss after num_examples_seen=286335 epoch=505: 1.251253\n",
      "2016-11-12 19:58:52: Loss after num_examples_seen=289170 epoch=510: 1.246029\n",
      "2016-11-12 19:59:18: Loss after num_examples_seen=292005 epoch=515: 1.240906\n",
      "2016-11-12 19:59:48: Loss after num_examples_seen=294840 epoch=520: 1.235322\n",
      "2016-11-12 20:00:14: Loss after num_examples_seen=297675 epoch=525: 1.232130\n",
      "2016-11-12 20:00:40: Loss after num_examples_seen=300510 epoch=530: 1.229555\n",
      "2016-11-12 20:01:02: Loss after num_examples_seen=303345 epoch=535: 1.226600\n",
      "2016-11-12 20:01:25: Loss after num_examples_seen=306180 epoch=540: 1.221819\n",
      "2016-11-12 20:01:48: Loss after num_examples_seen=309015 epoch=545: 1.214060\n",
      "2016-11-12 20:02:10: Loss after num_examples_seen=311850 epoch=550: 1.211679\n",
      "2016-11-12 20:02:33: Loss after num_examples_seen=314685 epoch=555: 1.204683\n",
      "2016-11-12 20:02:55: Loss after num_examples_seen=317520 epoch=560: 1.209086\n",
      "Setting learning rate to 0.000313\n",
      "2016-11-12 20:03:18: Loss after num_examples_seen=320355 epoch=565: 1.177001\n",
      "2016-11-12 20:03:40: Loss after num_examples_seen=323190 epoch=570: 1.167384\n",
      "2016-11-12 20:04:03: Loss after num_examples_seen=326025 epoch=575: 1.160053\n",
      "2016-11-12 20:04:25: Loss after num_examples_seen=328860 epoch=580: 1.153636\n",
      "2016-11-12 20:04:48: Loss after num_examples_seen=331695 epoch=585: 1.150167\n",
      "2016-11-12 20:05:11: Loss after num_examples_seen=334530 epoch=590: 1.153165\n",
      "Setting learning rate to 0.000156\n",
      "2016-11-12 20:05:33: Loss after num_examples_seen=337365 epoch=595: 1.113779\n",
      "2016-11-12 20:05:56: Loss after num_examples_seen=340200 epoch=600: 1.117333\n",
      "Setting learning rate to 0.000078\n",
      "2016-11-12 20:06:18: Loss after num_examples_seen=343035 epoch=605: 1.075880\n",
      "2016-11-12 20:06:41: Loss after num_examples_seen=345870 epoch=610: 1.056610\n",
      "2016-11-12 20:07:05: Loss after num_examples_seen=348705 epoch=615: 1.044338\n",
      "2016-11-12 20:07:29: Loss after num_examples_seen=351540 epoch=620: 1.031520\n",
      "2016-11-12 20:07:58: Loss after num_examples_seen=354375 epoch=625: 1.025435\n",
      "2016-11-12 20:08:33: Loss after num_examples_seen=357210 epoch=630: 1.017485\n",
      "2016-11-12 20:09:05: Loss after num_examples_seen=360045 epoch=635: 1.004378\n",
      "2016-11-12 20:09:37: Loss after num_examples_seen=362880 epoch=640: 0.998326\n",
      "2016-11-12 20:10:03: Loss after num_examples_seen=365715 epoch=645: 0.994611\n",
      "2016-11-12 20:10:31: Loss after num_examples_seen=368550 epoch=650: 0.987388\n",
      "2016-11-12 20:10:53: Loss after num_examples_seen=371385 epoch=655: 0.983051\n",
      "2016-11-12 20:11:16: Loss after num_examples_seen=374220 epoch=660: 0.971694\n",
      "2016-11-12 20:11:38: Loss after num_examples_seen=377055 epoch=665: 0.965751\n",
      "2016-11-12 20:12:00: Loss after num_examples_seen=379890 epoch=670: 0.958574\n",
      "2016-11-12 20:12:21: Loss after num_examples_seen=382725 epoch=675: 0.954347\n",
      "2016-11-12 20:12:43: Loss after num_examples_seen=385560 epoch=680: 0.950605\n",
      "2016-11-12 20:13:07: Loss after num_examples_seen=388395 epoch=685: 0.947403\n",
      "2016-11-12 20:13:30: Loss after num_examples_seen=391230 epoch=690: 0.942168\n",
      "2016-11-12 20:13:53: Loss after num_examples_seen=394065 epoch=695: 0.931582\n",
      "2016-11-12 20:14:14: Loss after num_examples_seen=396900 epoch=700: 0.927330\n",
      "2016-11-12 20:14:36: Loss after num_examples_seen=399735 epoch=705: 0.927674\n",
      "Setting learning rate to 0.000039\n",
      "2016-11-12 20:14:57: Loss after num_examples_seen=402570 epoch=710: 0.902274\n",
      "2016-11-12 20:15:20: Loss after num_examples_seen=405405 epoch=715: 0.896312\n",
      "2016-11-12 20:15:43: Loss after num_examples_seen=408240 epoch=720: 0.893758\n",
      "2016-11-12 20:16:05: Loss after num_examples_seen=411075 epoch=725: 0.889012\n",
      "2016-11-12 20:16:28: Loss after num_examples_seen=413910 epoch=730: 0.885849\n",
      "2016-11-12 20:16:50: Loss after num_examples_seen=416745 epoch=735: 0.884953\n",
      "2016-11-12 20:17:13: Loss after num_examples_seen=419580 epoch=740: 0.883965\n",
      "2016-11-12 20:17:36: Loss after num_examples_seen=422415 epoch=745: 0.884923\n",
      "Setting learning rate to 0.000020\n",
      "2016-11-12 20:18:00: Loss after num_examples_seen=425250 epoch=750: 0.878956\n",
      "2016-11-12 20:18:24: Loss after num_examples_seen=428085 epoch=755: 0.876285\n",
      "2016-11-12 20:18:47: Loss after num_examples_seen=430920 epoch=760: 0.875076\n",
      "2016-11-12 20:19:18: Loss after num_examples_seen=433755 epoch=765: 0.874562\n",
      "2016-11-12 20:19:41: Loss after num_examples_seen=436590 epoch=770: 0.874329\n",
      "2016-11-12 20:20:01: Loss after num_examples_seen=439425 epoch=775: 0.874207\n",
      "2016-11-12 20:20:26: Loss after num_examples_seen=442260 epoch=780: 0.874577\n",
      "Setting learning rate to 0.000010\n",
      "2016-11-12 20:20:48: Loss after num_examples_seen=445095 epoch=785: 0.872802\n",
      "2016-11-12 20:21:18: Loss after num_examples_seen=447930 epoch=790: 0.871896\n",
      "2016-11-12 20:21:46: Loss after num_examples_seen=450765 epoch=795: 0.871714\n",
      "2016-11-12 20:22:12: Loss after num_examples_seen=453600 epoch=800: 0.871735\n",
      "Setting learning rate to 0.000005\n",
      "2016-11-12 20:22:48: Loss after num_examples_seen=456435 epoch=805: 0.870763\n",
      "2016-11-12 20:23:25: Loss after num_examples_seen=459270 epoch=810: 0.870652\n",
      "2016-11-12 20:23:59: Loss after num_examples_seen=462105 epoch=815: 0.870681\n",
      "Setting learning rate to 0.000002\n",
      "2016-11-12 20:24:32: Loss after num_examples_seen=464940 epoch=820: 0.870943\n",
      "Setting learning rate to 0.000001\n",
      "2016-11-12 20:25:08: Loss after num_examples_seen=467775 epoch=825: 0.870811\n",
      "2016-11-12 20:25:44: Loss after num_examples_seen=470610 epoch=830: 0.870787\n",
      "2016-11-12 20:26:10: Loss after num_examples_seen=473445 epoch=835: 0.870831\n",
      "Setting learning rate to 0.000001\n",
      "2016-11-12 20:26:39: Loss after num_examples_seen=476280 epoch=840: 0.870885\n",
      "Setting learning rate to 0.000000\n",
      "2016-11-12 20:27:01: Loss after num_examples_seen=479115 epoch=845: 0.870925\n",
      "Setting learning rate to 0.000000\n",
      "2016-11-12 20:27:26: Loss after num_examples_seen=481950 epoch=850: 0.870949\n",
      "Setting learning rate to 0.000000\n",
      "2016-11-12 20:27:53: Loss after num_examples_seen=484785 epoch=855: 0.870962\n",
      "Setting learning rate to 0.000000\n",
      "2016-11-12 20:28:19: Loss after num_examples_seen=487620 epoch=860: 0.870968\n",
      "Setting learning rate to 0.000000\n",
      "2016-11-12 20:28:44: Loss after num_examples_seen=490455 epoch=865: 0.870972\n",
      "Setting learning rate to 0.000000\n",
      "2016-11-12 20:29:09: Loss after num_examples_seen=493290 epoch=870: 0.870974\n",
      "Setting learning rate to 0.000000\n",
      "2016-11-12 20:29:36: Loss after num_examples_seen=496125 epoch=875: 0.870974\n",
      "Setting learning rate to 0.000000\n",
      "2016-11-12 20:30:01: Loss after num_examples_seen=498960 epoch=880: 0.870975\n",
      "Setting learning rate to 0.000000\n",
      "2016-11-12 20:30:27: Loss after num_examples_seen=501795 epoch=885: 0.870975\n",
      "Setting learning rate to 0.000000\n",
      "2016-11-12 20:30:53: Loss after num_examples_seen=504630 epoch=890: 0.870975\n",
      "Setting learning rate to 0.000000\n",
      "2016-11-12 20:31:17: Loss after num_examples_seen=507465 epoch=895: 0.870975\n",
      "Setting learning rate to 0.000000\n",
      "2016-11-12 20:31:43: Loss after num_examples_seen=510300 epoch=900: 0.870975\n",
      "Setting learning rate to 0.000000\n",
      "2016-11-12 20:32:08: Loss after num_examples_seen=513135 epoch=905: 0.870975\n",
      "Setting learning rate to 0.000000\n",
      "2016-11-12 20:32:33: Loss after num_examples_seen=515970 epoch=910: 0.870975\n",
      "Setting learning rate to 0.000000\n",
      "2016-11-12 20:32:58: Loss after num_examples_seen=518805 epoch=915: 0.870975\n",
      "Setting learning rate to 0.000000\n",
      "2016-11-12 20:33:22: Loss after num_examples_seen=521640 epoch=920: 0.870975\n",
      "Setting learning rate to 0.000000\n",
      "2016-11-12 20:33:48: Loss after num_examples_seen=524475 epoch=925: 0.870975\n",
      "Setting learning rate to 0.000000\n",
      "2016-11-12 20:34:11: Loss after num_examples_seen=527310 epoch=930: 0.870975\n",
      "Setting learning rate to 0.000000\n",
      "2016-11-12 20:34:34: Loss after num_examples_seen=530145 epoch=935: 0.870975\n",
      "Setting learning rate to 0.000000\n",
      "2016-11-12 20:34:56: Loss after num_examples_seen=532980 epoch=940: 0.870975\n",
      "Setting learning rate to 0.000000\n",
      "2016-11-12 20:35:19: Loss after num_examples_seen=535815 epoch=945: 0.870975\n",
      "Setting learning rate to 0.000000\n",
      "2016-11-12 20:35:44: Loss after num_examples_seen=538650 epoch=950: 0.870975\n",
      "Setting learning rate to 0.000000\n",
      "2016-11-12 20:36:08: Loss after num_examples_seen=541485 epoch=955: 0.870975\n",
      "Setting learning rate to 0.000000\n",
      "2016-11-12 20:36:32: Loss after num_examples_seen=544320 epoch=960: 0.870975\n",
      "Setting learning rate to 0.000000\n",
      "2016-11-12 20:36:55: Loss after num_examples_seen=547155 epoch=965: 0.870975\n",
      "Setting learning rate to 0.000000\n",
      "2016-11-12 20:37:21: Loss after num_examples_seen=549990 epoch=970: 0.870975\n",
      "Setting learning rate to 0.000000\n",
      "2016-11-12 20:37:49: Loss after num_examples_seen=552825 epoch=975: 0.870975\n",
      "Setting learning rate to 0.000000\n",
      "2016-11-12 20:38:15: Loss after num_examples_seen=555660 epoch=980: 0.870975\n",
      "Setting learning rate to 0.000000\n",
      "2016-11-12 20:38:37: Loss after num_examples_seen=558495 epoch=985: 0.870975\n",
      "Setting learning rate to 0.000000\n",
      "2016-11-12 20:39:01: Loss after num_examples_seen=561330 epoch=990: 0.870975\n",
      "Setting learning rate to 0.000000\n",
      "2016-11-12 20:39:22: Loss after num_examples_seen=564165 epoch=995: 0.870975\n",
      "Setting learning rate to 0.000000\n"
     ]
    }
   ],
   "source": [
    "rnn.train(X_train_encode, Y_train_encode, step=0.005, nepoch=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x: a list of vectors (one hot encoded words)\n",
    "# return: a string\n",
    "def decode_to_sentence(x):\n",
    "    sent_t = [ vocab[one_hot_decode(e)][0] for e in x ] # a list of words (string)\n",
    "    return \" \".join(sent_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^ a cat may look at a king @\n",
      "[0, 3, 145, 71, 231, 57, 3, 383]\n"
     ]
    }
   ],
   "source": [
    "print sents[3]\n",
    "print X_train[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 1196)"
      ]
     },
     "execution_count": 666,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encode(X_train[3], voc_sz).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'^ a cat may look at a king'"
      ]
     },
     "execution_count": 667,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_to_sentence( one_hot_encode(X_train[3], voc_sz) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a cat may look at a king @'"
      ]
     },
     "execution_count": 668,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_to_sentence( one_hot_encode(Y_train[3], voc_sz) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(self):\n",
    "    x = one_hot_encode( [word_to_idx[sent_start]], self.dim ) # an encoded sentence\n",
    "    cnt = 0\n",
    "    while not one_hot_decode( x[-1] ) == word_to_idx[sent_end]:\n",
    "        o, s = self.forwardProp( x )  # o: T * dim\n",
    "        e = np.random.multinomial(1, o[-1]) # a sample from MN distri, e.g. [0, 0, 1, 0] for 4-choice multi-nomial.\n",
    "        x = np.append( x, [e], axis=0 )\n",
    "        cnt += 1\n",
    "        if cnt >= 30:  # avoid too long the sentence is\n",
    "            break \n",
    "    return decode_to_sentence( x )\n",
    "RNN.generate = generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^ do n't shoot the small @\n",
      "^ march abhors and april showers bring forth can flowers @\n",
      "^ dead men tell no tales @\n",
      "^ one half of the world does not know how the other half cherries @\n",
      "^ if it laughs n't broke , do n't @\n",
      "^ master begins at nine @\n",
      "^ to err is human ; to forgive divine @\n",
      "^ a picture paints a man words does @\n",
      "^ power corrupts ; absolute power corrupts absolutely @\n",
      "^ let not the sun go down on your wrath @\n",
      "^ see no evil , hear no evil , speak no evil @\n",
      "^ charity begins at multitude @\n",
      "^ the proof of the pudding is in you than cream @\n",
      "^ it 's no use crying over spilt milk @\n",
      "^ to err is human ; to forgive divine @\n",
      "^ the opera schemes not the cradle @\n",
      "^ he who can does , he who can not , teaches @\n",
      "^ you can choose your much but you ca n't choose your family @\n",
      "^ better to have loved and lost than never to have loved at all @\n",
      "^ distance heart enchantment to the view @\n"
     ]
    }
   ],
   "source": [
    "num_sents = 20\n",
    "for i in range(num_sents):\n",
    "    print rnn.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('rnn.pkl', 'wb') as f:\n",
    "    pickle.dump(rnn, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
