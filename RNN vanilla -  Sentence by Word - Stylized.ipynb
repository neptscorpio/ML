{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import itertools\n",
    "#nltk.download('all')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# softmax: take each COLUMN as a score vector\n",
    "# X: c x n np.ndarray\n",
    "# return a c x n ndarray\n",
    "# Caveat: if X is 1 * n array, it will apply on \"row\" still\n",
    "def softmax_by_col(X):\n",
    "    M = X.copy()\n",
    "    M -= M.max(axis=0)  # for each column, substract the max over row\n",
    "    eM = np.exp(M) \n",
    "    return eM / eM.sum(axis=0)  # softmax for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Caveat: if X is 1 * n array, it will return the same 1 * n array\n",
    "def softmax(X, by='col'):\n",
    "    if by == 'col':\n",
    "        return softmax_by_col(X)\n",
    "    elif by == 'row':\n",
    "        return softmax_by_col(X.T).T\n",
    "    else:\n",
    "        raise        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# y: nSamp x dim, each row is the real prob.\n",
    "# y_pred: nSamp x dim, each row is the predicted prob.\n",
    "# return: sum loss for all samples\n",
    "def sum_log_loss(y, y_pred):\n",
    "    return - ( y * np.log(y_pred + 1e-9) ).sum(axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.38521473638\n",
      "0.105360515658\n"
     ]
    }
   ],
   "source": [
    "y = np.asarray([[0., 1., 0., 0.], [0.3, 0.4, 0.2, 0.1]])\n",
    "y_pred = np.asarray([[0.1, 0.9, 0., 0.], [0.3, 0.4, 0.2, 0.1]])\n",
    "print sum_log_loss(y, y_pred)\n",
    "print -np.log(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x is a sequence/list of index (index in 0 - dim-1), 1 * n\n",
    "# return: e, n * dim\n",
    "def one_hot_encode(x, dim):\n",
    "    #print x, dim\n",
    "    n = len(x)\n",
    "    #print n\n",
    "    e = np.zeros( (n, dim) )\n",
    "    e[xrange(n), x] += 1\n",
    "    #print e.shape\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# e: an one-hot code, e.g. [0, 0, 1, 0]\n",
    "# return: index, e.g. 2\n",
    "def one_hot_decode(e):\n",
    "    return np.argmax(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x: a list of encoded vectors (one hot encoded words)\n",
    "# return: a string\n",
    "def decode_to_sentence(x):\n",
    "    sent_t = [ vocab[one_hot_decode(e)][0] for e in x ] # a list of words (string)\n",
    "    return \" \".join(sent_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voc_sz = 2000\n",
    "unknown = \"#\"\n",
    "sent_start = \"^\"\n",
    "sent_end = \"@\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training file...\n"
     ]
    }
   ],
   "source": [
    "# Read the data and append SENTENCE_START and SENTENCE_END tokens\n",
    "print \"Reading training file...\"\n",
    "with open('sayings.txt', 'rb') as f:\n",
    "    # body = f.read()\n",
    "    sents = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sents = [s.lower().rstrip() for s in sents if len(s)>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sents = [\"%s %s %s\" % (sent_start, s, sent_end) for s in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 567 sentences.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'^ a cat may look at a king @'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"Parsed %d sentences.\" % (len(sents))\n",
    "sents[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenize the sentences into words\n",
    "sents_t = [nltk.word_tokenize(s) for s in sents]  # a list of sent_t, i.e. list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example sentence: ^ a bad penny always turns up @\n",
      "\n",
      "Example tokenized sentence: ['^', 'work', 'expands', 'so', 'as', 'to', 'fill', 'the', 'time', 'available', '@']\n"
     ]
    }
   ],
   "source": [
    "print \"\\nExample sentence: %s\" % sents[0]\n",
    "print \"\\nExample tokenized sentence: %s\" % sents_t[546]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*sents_t)) # a FreqDist object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example word_freq itesm:  [('stones', 3), ('all', 32), ('forget', 1)]\n",
      "Found 1196 unique words tokens.\n"
     ]
    }
   ],
   "source": [
    "print \"Example word_freq itesm: \", word_freq.items()[:3]\n",
    "print \"Found %d unique words tokens.\" % len(word_freq.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voc_sz =  1196\n"
     ]
    }
   ],
   "source": [
    "voc_sz = min( [voc_sz, len(word_freq.items())] )\n",
    "print \"voc_sz = \", voc_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('^', 567), ('@', 567), ('the', 239), ('a', 183), ('is', 142), ('to', 84), (',', 84), ('you', 72), (\"'s\", 69), ('and', 68)]\n",
      "[('inspiration', 1), ('land', 1), ('calls', 1), ('wife', 1), ('age', 1), ('together', 1), ('squeaking', 1), ('greeks', 1), ('accidents', 1), ('expands', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common( voc_sz ) # a list of tuple (word, cnt)\n",
    "print vocab[:10]\n",
    "print vocab[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word -> an index in vocab, index in [0, voc_sz)\n",
    "word_to_idx = dict([(w, idx) for idx, (w, cnt) in enumerate(vocab)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n"
     ]
    }
   ],
   "source": [
    "print word_to_idx['^'], word_to_idx['@']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the training data\n",
    "# X: each row is a training sample, i.e., an input sequence, or, i.e., a sentence. Each word is an index (0 - voc_sz-1)\n",
    "# y: each row is a output, i.e., a shift-1-word sequence\n",
    "X_train = np.asarray([ [word_to_idx[w] for w in sent_t[:-1]] for sent_t in sents_t ])  # n sents_t\n",
    "Y_train = np.asarray([ [word_to_idx[w] for w in sent_t[1:]] for sent_t in sents_t ])   # n sents_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 3, 92, 136, 47, 511, 135] [0, 3, 436, 91, 19, 617]\n",
      " [0, 3, 182, 14, 2, 160, 4, 68, 69, 14, 2, 853]]\n",
      "[[3, 92, 136, 47, 511, 135, 1] [3, 436, 91, 19, 617, 1]\n",
      " [3, 182, 14, 2, 160, 4, 68, 69, 14, 2, 853, 1]]\n"
     ]
    }
   ],
   "source": [
    "print X_train[:3]\n",
    "print Y_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# one-hot encoding of word indices\n",
    "X_train_encode = []\n",
    "for x in X_train: # a sequence of indices\n",
    "    x_encode = one_hot_encode( x, voc_sz ) # T * dim\n",
    "    X_train_encode.append( x_encode )\n",
    "Y_train_encode = []\n",
    "for y in Y_train: # a sequence of indices\n",
    "    y_encode = one_hot_encode( y, voc_sz ) # T * dim\n",
    "    Y_train_encode.append( y_encode )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])]\n",
      "[array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.]]), array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.]])]\n"
     ]
    }
   ],
   "source": [
    "print X_train_encode[:2]\n",
    "print Y_train_encode[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valina RNN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN:\n",
    "     def __init__(self, dim, dim_h=100, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.dim = dim # dim. of input/output vec, e.g. the embedding vec. of a word in a sequence\n",
    "        self.dim_h = dim_h # hidden layer\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # U: input vec to hidden states, dim * dim_h, (so we can dot(x, U))\n",
    "        self.U = np.random.randn( dim, dim_h ) * np.sqrt( 2. / ( dim * dim_h ) )\n",
    "        # V: hidden states to output vec, dim_h * dim (so we can dot(V, h) )\n",
    "        self.V = np.random.randn( dim_h, dim ) * np.sqrt( 2. / ( dim_h * dim ) )\n",
    "        # W: (prev) hidden state to (next) hidden state \n",
    "        self.W = np.random.randn( dim_h, dim_h ) * np.sqrt( 2. / ( dim_h * dim_h ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# x: T * dim, a input sequence of vectors, [x1, x2, ..., x_t], each row is a 1 * dim vector\n",
    "# return: o, output softmax scores, T * dim\n",
    "#         s, hidden states, (T+1) * dim_h. s[-1] is initial state, all zeros\n",
    "def forwardProp(self, x):\n",
    "    T = len(x)\n",
    "    # Save all hidden states for bptt.\n",
    "    # Add one initial hidden state, which are 0\n",
    "    s = np.zeros( (T + 1, self.dim_h) ) # s[-1] are zeros (1 * dim_h)\n",
    "    o = np.zeros( (T, self.dim) )\n",
    "    for t in xrange(T):\n",
    "#         print \"t = \", t\n",
    "#         print \"x[t]\", x[t]\n",
    "#         print \"x[t].shap\", x[t].shape\n",
    "#         print \"U\", self.U.shape\n",
    "#         print \"s\", s[t-1].shape\n",
    "#         print \"W\", self.W.shape\n",
    "        s[t] = np.tanh( np.dot( x[t], self.U ) + np.dot( s[t-1], self.W ) ) # 1 * dim_h\n",
    "        o[t] = softmax( np.dot( s[t], self.V ), by='row' )  # 1 * dim \n",
    "    return [o, s]\n",
    "RNN.forwardProp = forwardProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x: T * dim, a input sequence of vectors, [x1, x2, ..., x_t], each row is a 1 * dim vector\n",
    "# return: labels, 1 * T, \n",
    "#         For each step t, label is the index of the dimension with max value in the output softmax score,\n",
    "def predict(self, x):\n",
    "    o, s = self.forwardProp(x)\n",
    "    return np.argmax(o, axis=1)\n",
    "RNN.predict = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X: a batch of n sequences, each sequence is a T * dim array. T may vary by sequence\n",
    "# Y: a batch of n sentences, each sequence is a T * dim array. T may vary by sequence\n",
    "# return: the sum log loss on all sequences\n",
    "def total_loss(self, X, Y):\n",
    "    n = len(X) # num. of training sequences\n",
    "    sum_loss = 0.\n",
    "    for i in xrange(n): # for each sequence\n",
    "        x = X[i]  # a sequence, T * dim\n",
    "        y = Y[i]  # a sequence, T * dim\n",
    "        o, s = self.forwardProp( x ) # T * dim\n",
    "        loss = sum_log_loss( y, o ) # sum loss in this sequence\n",
    "        sum_loss += loss\n",
    "    return sum_loss  # sum loss on all sequences of vectors\n",
    "RNN.total_loss = total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X: a batch of n sequences, each sequence is a T * dim array. T may vary by sequence\n",
    "# Y: a batch of n sentences, each sequence is a T * dim array. T may vary by sequence\n",
    "# return: the average log loss for each vector\n",
    "def loss(self, X, Y):\n",
    "    n = len(X) # num. of training sequences\n",
    "    sum_loss = 0.\n",
    "    for i in xrange(n): # for each sequence\n",
    "        x = X[i]  # a sequence, T * dim\n",
    "        #print x.shape\n",
    "        y = Y[i]  # a sequence, T * dim\n",
    "        #print y.shape\n",
    "        o, s = self.forwardProp( x ) # T * dim\n",
    "        #print o.shape\n",
    "        loss = sum_log_loss( y, o ) # sum loss in this sequence\n",
    "        #print loss\n",
    "        sum_loss += loss\n",
    "    n_vec = np.sum( [len(y) for y in Y] )  \n",
    "    #print n_vec\n",
    "    return sum_loss / n_vec  # avg. loss per vector\n",
    "RNN.loss = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### !!!  seeems only for one-hot encoding \n",
    "# x: a sequence of vec, T * dim\n",
    "# y: a sequence of vec, T * dim\n",
    "def bptt(self, x, y):\n",
    "    T = len(y)\n",
    "    o, s = self.forwardProp(x)  # o: T * dim,  s: T * dim_h\n",
    "    dU = np.zeros_like(self.U)  # dU is actually dL/dU\n",
    "    dV = np.zeros_like(self.V)\n",
    "    dW = np.zeros_like(self.W)\n",
    "    idx_x = np.argmax(x, axis=1) # 1 * T, each entry is an index\n",
    "    idx_y = np.argmax(y, axis=1) # 1 * T, each entry is an index\n",
    "    delta_o = o  # T * dim\n",
    "    delta_o[np.arange(len(idx_y)), idx_y] -= 1.\n",
    "    # For each output backwards...\n",
    "    for t in np.arange(T)[::-1]:\n",
    "        #print dV.shape  # dim_h * dim\n",
    "        dV += np.outer(s[t].T, delta_o[t])\n",
    "        # Initial delta calculation\n",
    "        delta_t = np.dot(delta_o[t], self.V.T) * (1 - (s[t] ** 2))   \n",
    "        # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "        steps = np.arange(max(0, t-self.bptt_truncate), t+1)\n",
    "        for bptt_step in steps[::-1]:\n",
    "            dW += np.outer(s[bptt_step-1].T, delta_t)              \n",
    "            dU[idx_x[bptt_step], :] += delta_t\n",
    "            # Update delta for next step\n",
    "            delta_t = np.dot(delta_t, self.W.T) * (1 - s[bptt_step-1] ** 2)\n",
    "    return [dU, dV, dW]\n",
    "RNN.bptt = bptt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "# x: a sequence of vec, T * dim\n",
    "# y: a sequence of vec, T * dim\n",
    "def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "    # Calculate the gradients using backpropagation. We want to checker if these are correct.\n",
    "    bptt_gradients = self.bptt(x, y)\n",
    "    # List of all parameters we want to check.\n",
    "    model_parameters = ['U', 'V', 'W']\n",
    "    # Gradient check for each parameter\n",
    "    for pidx, pname in enumerate(model_parameters):\n",
    "        # Get the actual parameter value from the mode, e.g. model.W\n",
    "        parameter = operator.attrgetter(pname)(self)\n",
    "        print \"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape))\n",
    "        # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "        it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            # Save the original value so we can reset it later\n",
    "            original_value = parameter[ix]\n",
    "            # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "            parameter[ix] = original_value + h\n",
    "            #print [x].shape\n",
    "            #print [y].shape\n",
    "            gradplus = self.total_loss([x],[y])\n",
    "            parameter[ix] = original_value - h\n",
    "            gradminus = self.total_loss([x],[y])\n",
    "            estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "            # Reset parameter to original value\n",
    "            parameter[ix] = original_value\n",
    "            # The gradient for this parameter calculated using backpropagation\n",
    "            backprop_gradient = bptt_gradients[pidx][ix]\n",
    "            # calculate The relative error: (|x - y|/(|x| + |y|))\n",
    "            relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "            # If the error is to large fail the gradient check\n",
    "            if relative_error >= error_threshold:\n",
    "                print \"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix)\n",
    "                print \"+h Loss: %f\" % gradplus\n",
    "                print \"-h Loss: %f\" % gradminus\n",
    "                print \"Estimated_gradient: %f\" % estimated_gradient\n",
    "                print \"Backpropagation gradient: %f\" % backprop_gradient\n",
    "                print \"Relative Error: %f\" % relative_error\n",
    "                return\n",
    "            it.iternext()\n",
    "        print \"Gradient check for parameter %s passed.\" % (pname)\n",
    "RNN.gradient_check = gradient_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To avoid performing millions of expensive calculations we use a smaller vocabulary size for checking.\n",
    "if 0:\n",
    "    grad_check_vocab_size = 10\n",
    "    np.random.seed(10)\n",
    "    model = RNN(grad_check_vocab_size, 5, bptt_truncate=1000)\n",
    "\n",
    "    #print \"U\", model.U.shape\n",
    "    model.gradient_check( one_hot_encode( [0,1,2,3], grad_check_vocab_size), \n",
    "                          one_hot_encode( [1,2,3,4], grad_check_vocab_size)\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Performs one step of SGD on 1 training sequence\n",
    "# x: a sequence of vec, T * dim\n",
    "# y: a sequence of vec, T * dim\n",
    "def sgd_update(self, x, y, step):\n",
    "    # Calculate the gradients\n",
    "    dU, dV, dW = self.bptt(x, y)\n",
    "    # Change parameters according to gradients and learning rate\n",
    "    self.U += - step * dU\n",
    "    self.V += - step * dV\n",
    "    self.W += - step * dW\n",
    "RNN.sgd_update = sgd_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# Training Loop\n",
    "# - X_train: The training data set, batch of sequences\n",
    "# - y_train: The training data labels, batch\n",
    "# - step: learning rate for SGD\n",
    "# - nepoch: Number of times to iterate through the complete dataset\n",
    "def train(self, X_train_encode, Y_train_encode, step=0.005, nepoch=100):\n",
    "    losses = []     # keep track of the losses\n",
    "    n_sample_seen = 0  # samples (sequences) have seen\n",
    "    for epoch in range(nepoch):\n",
    "        if (epoch % 5 == 0):\n",
    "            loss = self.loss(X_train_encode, Y_train_encode)\n",
    "            losses.append( (n_sample_seen, loss) )\n",
    "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print \"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, n_sample_seen, epoch, loss)\n",
    "            # Adjust the learning rate if loss increases\n",
    "            if ( len(losses) >= 2 and losses[-1][1] >= losses[-2][1]):\n",
    "                step *= 0.5 \n",
    "                print \"Setting learning rate to %f\" % step\n",
    "\n",
    "        n = len(Y_train_encode)\n",
    "        for i in xrange(n): # train sample by sample, i.e. batch=1\n",
    "            self.sgd_update(X_train_encode[i], Y_train_encode[i], step)\n",
    "            n_sample_seen += 1\n",
    "RNN.train = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(self):\n",
    "    x = one_hot_encode( [word_to_idx[sent_start]], self.dim ) # an encoded sentence\n",
    "    cnt = 0\n",
    "    while not one_hot_decode( x[-1] ) == word_to_idx[sent_end]:\n",
    "        o, s = self.forwardProp( x )  # o: T * dim\n",
    "        e = np.random.multinomial(1, o[-1]) # a sample from MN distri, e.g. [0, 0, 1, 0] for 4-choice multi-nomial.\n",
    "        x = np.append( x, [e], axis=0 )\n",
    "        cnt += 1\n",
    "        if cnt >= 30:  # avoid too long the sentence is\n",
    "            break \n",
    "    return decode_to_sentence( x )\n",
    "RNN.generate = generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1196\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "print voc_sz\n",
    "rnn = RNN(dim=voc_sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A test sgd step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 12.1 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit rnn.sgd_update(X_train_encode[10], Y_train_encode[10], step=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train (can repeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-11-12 16:03:20: Loss after num_examples_seen=0 epoch=0: 7.080235\n",
      "2016-11-12 16:03:58: Loss after num_examples_seen=2835 epoch=5: 5.418642\n"
     ]
    }
   ],
   "source": [
    "rnn.train(X_train_encode, Y_train_encode, step=0.005, nepoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 0:\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d-%H:%M:%S')\n",
    "    with open('rnn_%s.pkl' % timestamp, 'wb') as f:\n",
    "        pickle.dump(rnn, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 0:\n",
    "    import pickle\n",
    "    with open('rnn.pkl', 'rb') as f:\n",
    "        rnn = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^ attack is the best form @\n",
      "^ saturday 's child works hard for its living , @\n",
      "^ the labourer is worthy of his hire @\n",
      "^ never judge a book by its evil @\n",
      "^ distance wise enchantment to the view @\n",
      "^ for want of a nail the shoe was lost ; for want of a shoe the man was lost ; and worth want of up with a shoe in everything\n",
      "^ a person is known by the company he keeps @\n",
      "^ god vessels those who most themselves @\n",
      "^ parsley half goes nine @\n",
      "^ flattery will get you nowhere @\n",
      "^ ask a silly question and you 'll get a sow answer @\n",
      "^ no man can serve two masters @\n",
      "^ every little helps two us to suck @\n",
      "^ the course of sin is a season forever @\n",
      "^ as you make your bed , so you must lie upon it @\n",
      "^ give a dog a bad head and hang him @\n",
      "^ it never easy but it the same to sings no pin in the going @\n",
      "^ one hand washes the other @\n",
      "^ all 's fair that love and suspicion @\n",
      "^ you ca n't make with the hare and red sky @\n"
     ]
    }
   ],
   "source": [
    "num_sents = 20\n",
    "for i in range(num_sents):\n",
    "    print rnn.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
